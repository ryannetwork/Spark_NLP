{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling using Spark NLP and Spark MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download the dataset from the following kaggle link,\n",
    "\n",
    "https://github.com/ravishchawla/topic_modeling/blob/master/data/abcnews-date-text.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using Spark NLP for preprocessing the data. Spark NLP is a NLP library for performing various text proprocessing operations that are required to clean the text. For more information, check out the website. \n",
    "\n",
    "https://nlp.johnsnowlabs.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using Spark MLlib's LDA model for extracting the topics from the dataset. Let's go ahead and load the data. Before loading the data, lets import all the required libraries and initialize spark session. As I am using single node spark, we need to allocation memory according to the host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Spark NLP\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "import sparknlp\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Start Spark Session with Spark NLP\n",
    "#spark = sparknlp.start()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\")\\\n",
    "    .master(\"local[4]\")\\\n",
    "    .config(\"spark.driver.memory\",\"8G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2G\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.5\")\\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1000M\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data into Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1041793"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# File location and type\n",
    "file_location = r'E:\\Machine Learning\\data\\abcnews_date_txt.csv'\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "\n",
    "df = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)\n",
    "\n",
    "# Verify the count\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[publish_date: int, headline_text: string]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify the schema. We used inferschema = True.\n",
    "\n",
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|publish_date|       headline_text|\n",
      "+------------+--------------------+\n",
      "|    20030219|aba decides again...|\n",
      "|    20030219|act fire witnesse...|\n",
      "|    20030219|a g calls for inf...|\n",
      "|    20030219|air nz staff in a...|\n",
      "|    20030219|air nz strike to ...|\n",
      "|    20030219|ambitious olsson ...|\n",
      "|    20030219|antic delighted w...|\n",
      "|    20030219|aussie qualifier ...|\n",
      "|    20030219|aust addresses un...|\n",
      "|    20030219|australia is lock...|\n",
      "|    20030219|australia to cont...|\n",
      "|    20030219|barca take record...|\n",
      "|    20030219|bathhouse plans m...|\n",
      "|    20030219|big hopes for lau...|\n",
      "|    20030219|big plan to boost...|\n",
      "|    20030219|blizzard buries u...|\n",
      "|    20030219|brigadier dismiss...|\n",
      "|    20030219|british combat tr...|\n",
      "|    20030219|bryant leads lake...|\n",
      "|    20030219|bushfire victims ...|\n",
      "+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data looks good and the count is also adequate for the example. Lets go ahead and build our NLP pipeline using Spark NLP,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|publish_date|              tokens|\n",
      "+------------+--------------------+\n",
      "|    20030219|[aba, decid, comm...|\n",
      "|    20030219|[act, fire, wit, ...|\n",
      "|    20030219|[g, call, infrast...|\n",
      "|    20030219|[air, nz, staff, ...|\n",
      "|    20030219|[air, nz, strike,...|\n",
      "|    20030219|[ambiti, olsson, ...|\n",
      "|    20030219|[antic, delight, ...|\n",
      "|    20030219|[aussi, qualifi, ...|\n",
      "|    20030219|[aust, address, u...|\n",
      "|    20030219|[australia, lock,...|\n",
      "|    20030219|[australia, contr...|\n",
      "|    20030219|[barca, take, rec...|\n",
      "|    20030219|[bathhous, plan, ...|\n",
      "|    20030219|[big, hope, launc...|\n",
      "|    20030219|[big, plan, boost...|\n",
      "|    20030219|[blizzard, buri, ...|\n",
      "|    20030219|[brigadi, dismiss...|\n",
      "|    20030219|[british, combat,...|\n",
      "|    20030219|[bryant, lead, la...|\n",
      "|    20030219|[bushfir, victim,...|\n",
      "+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Spark NLp requires the input dataframe or column to be converted to document. \n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"headline_text\") \\\n",
    "    .setOutputCol(\"document\") \\\n",
    "    .setCleanupMode(\"shrink\")\n",
    "\n",
    "# Split sentence to tokens(array)\n",
    "tokenizer = Tokenizer() \\\n",
    "  .setInputCols([\"document\"]) \\\n",
    "  .setOutputCol(\"token\")\n",
    "\n",
    "# clean unwanted characters and garbage\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"normalized\")\n",
    "\n",
    "# remove stopwords\n",
    "stopwords_cleaner = StopWordsCleaner()\\\n",
    "      .setInputCols(\"normalized\")\\\n",
    "      .setOutputCol(\"cleanTokens\")\\\n",
    "      .setCaseSensitive(False)\n",
    "\n",
    "# stem the words to bring them to the root form.\n",
    "stemmer = Stemmer() \\\n",
    "    .setInputCols([\"cleanTokens\"]) \\\n",
    "    .setOutputCol(\"stem\")\n",
    "\n",
    "# Finisher is the most important annotator. Spark NLP adds its own structure when we convert each row in the dataframe \n",
    "# to document. Finisher helps us to bring back the expected structure viz. array of tokens.\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"stem\"]) \\\n",
    "    .setOutputCols([\"tokens\"]) \\\n",
    "    .setOutputAsArray(True) \\\n",
    "    .setCleanAnnotations(False)\n",
    "\n",
    "# We build a ml pipeline so that each phase can be executed in sequence. This pipeline can also be used to test the model. \n",
    "nlp_pipeline = Pipeline(\n",
    "    stages=[document_assembler, \n",
    "            tokenizer,\n",
    "            normalizer,\n",
    "            stopwords_cleaner, \n",
    "            stemmer, \n",
    "            finisher])\n",
    "\n",
    "# train the pipeline\n",
    "nlp_model = nlp_pipeline.fit(df)\n",
    "\n",
    "# apply the pipeline to transform dataframe.\n",
    "processed_df  = nlp_model.transform(df)\n",
    "\n",
    "\n",
    "tokens_df = processed_df.select('publish_date','tokens').limit(10000)\n",
    "\n",
    "tokens_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# LDA model expects a vector of token counts. So we will use Countvectorizer from spark mllib.\n",
    "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"features\", vocabSize=500, minDF=3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_model = cv.fit(tokens_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_tokens = cv_model.transform(tokens_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lower bound on the log likelihood of the entire corpus: -179294.34056462505\n",
      "The upper bound on perplexity: 6.319411411413543\n",
      "The topics described by their top-weighted terms:\n",
      "+-----+-----------+------------------------------------------------------------------+\n",
      "|topic|termIndices|termWeights                                                       |\n",
      "+-----+-----------+------------------------------------------------------------------+\n",
      "|0    |[4, 2, 16] |[0.02316016397290078, 0.018882193389718523, 0.01638984506775528]  |\n",
      "|1    |[1, 6, 8]  |[0.023489583320613704, 0.021933736786838697, 0.019705093722386827]|\n",
      "|2    |[0, 2, 15] |[0.03184761923877096, 0.016924401517207907, 0.016542356800711155] |\n",
      "+-----+-----------+------------------------------------------------------------------+\n",
      "\n",
      "+------------+---------------------------------------------------+---------------------------------------------------------------+--------------------------------------------------------------+\n",
      "|publish_date|tokens                                             |features                                                       |topicDistribution                                             |\n",
      "+------------+---------------------------------------------------+---------------------------------------------------------------+--------------------------------------------------------------+\n",
      "|20030219    |[aba, decid, commun, broadcast, licenc]            |(500,[118,498],[1.0,1.0])                                      |[0.7507198996634286,0.11557758553359523,0.1337025148029763]   |\n",
      "|20030219    |[act, fire, wit, must, awar, defam]                |(500,[12,116,389],[1.0,1.0,1.0])                               |[0.30375930288268777,0.4268064738709402,0.269434223246372]    |\n",
      "|20030219    |[g, call, infrastructur, protect, summit]          |(500,[14,444],[1.0,1.0])                                       |[0.11777885588337302,0.11509238413861504,0.7671287599780119]  |\n",
      "|20030219    |[air, nz, staff, aust, strike, pai, rise]          |(500,[59,61,112,117,152,292,475],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])|[0.047879109448221185,0.6278988993722384,0.3242219911795402]  |\n",
      "|20030219    |[air, nz, strike, affect, australian, travel]      |(500,[61,93,112,292],[1.0,1.0,1.0,1.0])                        |[0.08751657854919224,0.5321803019475277,0.38030311950328]     |\n",
      "|20030219    |[ambiti, olsson, win, tripl, jump]                 |(500,[11],[1.0])                                               |[0.17041834347429283,0.6595894210106439,0.16999223551506332]  |\n",
      "|20030219    |[antic, delight, record, break, barca]             |(500,[120,254],[1.0,1.0])                                      |[0.12439548173726221,0.7592277271604586,0.11637679110227926]  |\n",
      "|20030219    |[aussi, qualifi, stosur, wast, four, memphi, match]|(500,[174,189,222,471,499],[1.0,1.0,1.0,1.0,1.0])              |[0.531051060476722,0.3874153630490605,0.08153357647421745]    |\n",
      "|20030219    |[aust, address, un, secur, council, iraq]          |(500,[1,10,43,59,68],[1.0,1.0,1.0,1.0,1.0])                    |[0.058705659478779565,0.883080646715083,0.058213693806137556] |\n",
      "|20030219    |[australia, lock, war, timet, opp]                 |(500,[2,58,418],[1.0,1.0,1.0])                                 |[0.8074386038958429,0.10102425374964387,0.09153714235451331]  |\n",
      "|20030219    |[australia, contribut, million, aid, iraq]         |(500,[1,58,173,429],[1.0,1.0,1.0,1.0])                         |[0.07513612669917802,0.8534324106015136,0.07143146269930822]  |\n",
      "|20030219    |[barca, take, record, robson, celebr, birthdai]    |(500,[33,120],[1.0,1.0])                                       |[0.15750647013228072,0.3832851925736552,0.459208337294064]    |\n",
      "|20030219    |[bathhous, plan, move, ahead]                      |(500,[6,82,221],[1.0,1.0,1.0])                                 |[0.08797980626164689,0.6763790537757101,0.2356411399626431]   |\n",
      "|20030219    |[big, hope, launceston, cycl, championship]        |(500,[41,280,393],[1.0,1.0,1.0])                               |[0.0893689944210039,0.4479906988762964,0.4626403067026997]    |\n",
      "|20030219    |[big, plan, boost, paroo, water, suppli]           |(500,[6,30,57,280,330],[1.0,1.0,1.0,1.0,1.0])                  |[0.057646760163274136,0.8799390004652933,0.062414239371432575]|\n",
      "|20030219    |[blizzard, buri, unit, state, bill]                |(500,[275,281,387],[1.0,1.0,1.0])                              |[0.7914335096989922,0.09750219546019227,0.11106429484081558]  |\n",
      "|20030219    |[brigadi, dismiss, report, troop, harass]          |(500,[19,32,461],[1.0,1.0,1.0])                                |[0.10904259756287904,0.5791989417440322,0.31175846069308877]  |\n",
      "|20030219    |[british, combat, troop, arriv, daili, kuwait]     |(500,[32,134,487],[1.0,1.0,1.0])                               |[0.09911074389240851,0.5674510223343316,0.33343823377326]     |\n",
      "|20030219    |[bryant, lead, laker, doubl, overtim, win]         |(500,[11,52,314],[1.0,1.0,1.0])                                |[0.1548263530297398,0.7325545574637373,0.11261908950652279]   |\n",
      "|20030219    |[bushfir, victim, urg, see, centrelink]            |(500,[24,138,208],[1.0,1.0,1.0])                               |[0.45234183781134596,0.45284904696775374,0.0948091152209004]  |\n",
      "+------------+---------------------------------------------------+---------------------------------------------------------------+--------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "num_topics = 3\n",
    "\n",
    "lda = LDA(k=num_topics, maxIter=10)\n",
    "model = lda.fit(vectorized_tokens)\n",
    "\n",
    "ll = model.logLikelihood(vectorized_tokens)\n",
    "lp = model.logPerplexity(vectorized_tokens)\n",
    "\n",
    "print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
    "print(\"The upper bound on perplexity: \" + str(lp))\n",
    "\n",
    "# Describe topics.\n",
    "topics = model.describeTopics(3)\n",
    "print(\"The topics described by their top-weighted terms:\")\n",
    "topics.show(truncate=False)\n",
    "\n",
    "# Shows the result\n",
    "transformed = model.transform(vectorized_tokens)\n",
    "transformed.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting words to visualize the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 0\n",
      "----------\n",
      "govt\n",
      "war\n",
      "protest\n",
      "iraq\n",
      "polic\n",
      "continu\n",
      "warn\n",
      "new\n",
      "world\n",
      "rain\n",
      "----------\n",
      "topic: 1\n",
      "----------\n",
      "iraq\n",
      "plan\n",
      "sai\n",
      "council\n",
      "win\n",
      "u\n",
      "claim\n",
      "report\n",
      "urg\n",
      "water\n",
      "----------\n",
      "topic: 2\n",
      "----------\n",
      "u\n",
      "war\n",
      "charg\n",
      "man\n",
      "polic\n",
      "call\n",
      "fire\n",
      "take\n",
      "kill\n",
      "death\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "vocab = cv_model.vocabulary\n",
    "\n",
    "topics = model.describeTopics()   \n",
    "topics_rdd = topics.rdd\n",
    "\n",
    "topics_words = topics_rdd\\\n",
    "       .map(lambda row: row['termIndices'])\\\n",
    "       .map(lambda idx_list: [vocab[idx] for idx in idx_list])\\\n",
    "       .collect()\n",
    "\n",
    "for idx, topic in enumerate(topics_words):\n",
    "    print(\"topic: {}\".format(idx))\n",
    "    print(\"----------\")\n",
    "    for word in topic:\n",
    "       print(word)\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above pipeline is a basic pipeline but one can custome it according to the need. For e.g. you can add Ngrams and see how the topics change (may get more meaning full topics)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
